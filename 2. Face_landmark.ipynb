{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Face landmarks => 2 eyes, 1 nose, 2 ears, 1 lip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to parse: node {\n  name: \"facedetectionshortrange__facedetection__ToImageCalculator\"\n  calculator: \"ToImageCalculator\"\n  input_stream: \"IMAGE:image\"\n  output_stream: \"IMAGE:facedetectionshortrange__facedetection__multi_backend_image\"\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__ImageToTensorCalculator\"\n  calculator: \"ImageToTensorCalculator\"\n  input_stream: \"IMAGE:facedetectionshortrange__facedetection__multi_backend_image\"\n  output_stream: \"TENSORS:facedetectionshortrange__facedetection__input_tensors\"\n  output_stream: \"MATRIX:facedetectionshortrange__facedetection__transform_matrix\"\n  node_options {\n    type_url: \"type.googleapis.com/mediapipe.ImageToTensorCalculatorOptions\"\n    value: \"\\030\\001\"\\n\\r\\000\\000\\200\\277\\025\\000\\000\\200?0\\001\\010\\200\\001\\020\\200\\001\"\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__SsdAnchorsCalculator\"\n  calculator: \"SsdAnchorsCalculator\"\n  output_side_packet: \"facedetectionshortrange__facedetection__anchors\"\n  node_options {\n    type_url: \"type.googleapis.com/mediapipe.SsdAnchorsCalculatorOptions\"\n    value: \"\\035\\000\\000\\030>%\\000\\000@?-\\000\\000\\000?5\\000\\000\\000?]\\000\\000\\200?p\\001\\010\\200\\001\\020\\200\\0018\\004P\\010P\\020P\\020P\\020m\\000\\000\\200?\"\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__inferencecalculator__facedetectionshortrange__facedetection__InferenceCalculator\"\n  calculator: \"InferenceCalculatorCpu\"\n  input_stream: \"TENSORS:facedetectionshortrange__facedetection__input_tensors\"\n  output_stream: \"TENSORS:facedetectionshortrange__facedetection__detection_tensors\"\n  node_options {\n    type_url: \"type.googleapis.com/mediapipe.InferenceCalculatorOptions\"\n    value: \"*\\002\"\\000\\nBmediapipe/modules/face_detection/face_detection_short_range.tflite\"\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__TensorsToDetectionsCalculator\"\n  calculator: \"TensorsToDetectionsCalculator\"\n  input_stream: \"TENSORS:facedetectionshortrange__facedetection__detection_tensors\"\n  output_stream: \"DETECTIONS:facedetectionshortrange__facedetection__unfiltered_detections\"\n  input_side_packet: \"ANCHORS:facedetectionshortrange__facedetection__anchors\"\n  node_options {\n    type_url: \"type.googleapis.com/mediapipe.TensorsToDetectionsCalculatorOptions\"\n    value: \"\\010\\001\\030\\020H\\004P\\006X\\002`\\000p\\001x\\001\\205\\001\\000\\000\\310B\\020\\200\\007%\\000\\000\\000C-\\000\\000\\000C=\\000\\000\\000C5\\000\\000\\000C\\235\\001\\000\\000\\000?\"\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__NonMaxSuppressionCalculator\"\n  calculator: \"NonMaxSuppressionCalculator\"\n  input_stream: \"facedetectionshortrange__facedetection__unfiltered_detections\"\n  output_stream: \"facedetectionshortrange__facedetection__filtered_detections\"\n  options {\n    [mediapipe.NonMaxSuppressionCalculatorOptions.ext] {\n      min_suppression_threshold: 0.3\n      overlap_type: INTERSECTION_OVER_UNION\n      algorithm: WEIGHTED\n    }\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__DetectionProjectionCalculator\"\n  calculator: \"DetectionProjectionCalculator\"\n  input_stream: \"DETECTIONS:facedetectionshortrange__facedetection__filtered_detections\"\n  input_stream: \"PROJECTION_MATRIX:facedetectionshortrange__facedetection__transform_matrix\"\n  output_stream: \"DETECTIONS:detections\"\n}\ninput_stream: \"IMAGE:image\"\nexecutor {\n}\noutput_stream: \"DETECTIONS:detections\"\ntype: \"FaceDetectionShortRangeCpu\"\ngraph_options {\n  type_url: \"type.googleapis.com/mediapipe.FaceDetectionOptions\"\n  value: \"\\245\\002\\000\\000\\000?\"\n}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m face_detect \u001b[38;5;241m=\u001b[39m \u001b[43mmp_detection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFaceDetection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mediapipe\\python\\solutions\\face_detection.py:82\u001b[0m, in \u001b[0;36mFaceDetection.__init__\u001b[1;34m(self, min_detection_confidence, model_selection)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initializes a MediaPipe Face Detection object.\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03m    https://solutions.mediapipe.dev/face_detection#model_selection.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m binary_graph_path \u001b[38;5;241m=\u001b[39m _FULL_RANGE_GRAPH_FILE_PATH \u001b[38;5;28;01mif\u001b[39;00m model_selection \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m _SHORT_RANGE_GRAPH_FILE_PATH\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbinary_graph_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary_graph_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_graph_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mface_detection_pb2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFaceDetectionOptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin_score_thresh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_detection_confidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdetections\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\mediapipe\\python\\solution_base.py:248\u001b[0m, in \u001b[0;36mSolutionBase.__init__\u001b[1;34m(self, binary_graph_path, graph_config, calculator_params, graph_options, side_inputs, outputs, stream_type_hints)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m graph_options:\n\u001b[0;32m    245\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_extension(canonical_graph_config_proto\u001b[38;5;241m.\u001b[39mgraph_options,\n\u001b[0;32m    246\u001b[0m                       graph_options)\n\u001b[1;32m--> 248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;241m=\u001b[39m \u001b[43mcalculator_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCalculatorGraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcanonical_graph_config_proto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_outputs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to parse: node {\n  name: \"facedetectionshortrange__facedetection__ToImageCalculator\"\n  calculator: \"ToImageCalculator\"\n  input_stream: \"IMAGE:image\"\n  output_stream: \"IMAGE:facedetectionshortrange__facedetection__multi_backend_image\"\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__ImageToTensorCalculator\"\n  calculator: \"ImageToTensorCalculator\"\n  input_stream: \"IMAGE:facedetectionshortrange__facedetection__multi_backend_image\"\n  output_stream: \"TENSORS:facedetectionshortrange__facedetection__input_tensors\"\n  output_stream: \"MATRIX:facedetectionshortrange__facedetection__transform_matrix\"\n  node_options {\n    type_url: \"type.googleapis.com/mediapipe.ImageToTensorCalculatorOptions\"\n    value: \"\\030\\001\"\\n\\r\\000\\000\\200\\277\\025\\000\\000\\200?0\\001\\010\\200\\001\\020\\200\\001\"\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__SsdAnchorsCalculator\"\n  calculator: \"SsdAnchorsCalculator\"\n  output_side_packet: \"facedetectionshortrange__facedetection__anchors\"\n  node_options {\n    type_url: \"type.googleapis.com/mediapipe.SsdAnchorsCalculatorOptions\"\n    value: \"\\035\\000\\000\\030>%\\000\\000@?-\\000\\000\\000?5\\000\\000\\000?]\\000\\000\\200?p\\001\\010\\200\\001\\020\\200\\0018\\004P\\010P\\020P\\020P\\020m\\000\\000\\200?\"\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__inferencecalculator__facedetectionshortrange__facedetection__InferenceCalculator\"\n  calculator: \"InferenceCalculatorCpu\"\n  input_stream: \"TENSORS:facedetectionshortrange__facedetection__input_tensors\"\n  output_stream: \"TENSORS:facedetectionshortrange__facedetection__detection_tensors\"\n  node_options {\n    type_url: \"type.googleapis.com/mediapipe.InferenceCalculatorOptions\"\n    value: \"*\\002\"\\000\\nBmediapipe/modules/face_detection/face_detection_short_range.tflite\"\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__TensorsToDetectionsCalculator\"\n  calculator: \"TensorsToDetectionsCalculator\"\n  input_stream: \"TENSORS:facedetectionshortrange__facedetection__detection_tensors\"\n  output_stream: \"DETECTIONS:facedetectionshortrange__facedetection__unfiltered_detections\"\n  input_side_packet: \"ANCHORS:facedetectionshortrange__facedetection__anchors\"\n  node_options {\n    type_url: \"type.googleapis.com/mediapipe.TensorsToDetectionsCalculatorOptions\"\n    value: \"\\010\\001\\030\\020H\\004P\\006X\\002`\\000p\\001x\\001\\205\\001\\000\\000\\310B\\020\\200\\007%\\000\\000\\000C-\\000\\000\\000C=\\000\\000\\000C5\\000\\000\\000C\\235\\001\\000\\000\\000?\"\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__NonMaxSuppressionCalculator\"\n  calculator: \"NonMaxSuppressionCalculator\"\n  input_stream: \"facedetectionshortrange__facedetection__unfiltered_detections\"\n  output_stream: \"facedetectionshortrange__facedetection__filtered_detections\"\n  options {\n    [mediapipe.NonMaxSuppressionCalculatorOptions.ext] {\n      min_suppression_threshold: 0.3\n      overlap_type: INTERSECTION_OVER_UNION\n      algorithm: WEIGHTED\n    }\n  }\n}\nnode {\n  name: \"facedetectionshortrange__facedetection__DetectionProjectionCalculator\"\n  calculator: \"DetectionProjectionCalculator\"\n  input_stream: \"DETECTIONS:facedetectionshortrange__facedetection__filtered_detections\"\n  input_stream: \"PROJECTION_MATRIX:facedetectionshortrange__facedetection__transform_matrix\"\n  output_stream: \"DETECTIONS:detections\"\n}\ninput_stream: \"IMAGE:image\"\nexecutor {\n}\noutput_stream: \"DETECTIONS:detections\"\ntype: \"FaceDetectionShortRangeCpu\"\ngraph_options {\n  type_url: \"type.googleapis.com/mediapipe.FaceDetectionOptions\"\n  value: \"\\245\\002\\000\\000\\000?\"\n}\n"
     ]
    }
   ],
   "source": [
    "face_detect = mp_detection.FaceDetection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'face_detect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m suc,img \u001b[38;5;241m=\u001b[39m video\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      4\u001b[0m img1 \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mface_detect\u001b[49m\u001b[38;5;241m.\u001b[39mprocess(img1)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# here the name od landmark is detections\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'face_detect' is not defined"
     ]
    }
   ],
   "source": [
    "video = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    suc,img = video.read()\n",
    "    img1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    result = face_detect.process(img1)\n",
    "    print(result)\n",
    "    # here the name od landmark is detections\n",
    "    print(result.detections)\n",
    "\n",
    "    # show landmark only when face is detected\n",
    "    if result.detections:\n",
    "        for i in result.detections:\n",
    "            mp_drawing.draw_detection(img,i)\n",
    "\n",
    "\n",
    "    cv2.imshow(\"image\",img)\n",
    "    if cv2.waitKey(1) & 0XFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
